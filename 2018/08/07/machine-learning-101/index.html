<!DOCTYPE html><html><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Machine Learning Refresher</title><link rel="shortcut icon" href="/images/avatar.png"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,400,700"><link rel="stylesheet" href="//cdn.bootcss.com/highlight.js/9.2.0/styles/github.min.css"><script src="//cdn.bootcss.com/highlight.js/9.2.0/highlight.min.js"></script></head><body><nav class="main-nav"><a href="/">Home</a><a href="/archives">Archives</a><a href="/books">Book log</a></nav><div class="profile"><section id="wrapper"><header id="header"><a href="/"><img class="2x" id="avatar" src="/images/avatar.png"></a><h1>niek's notes</h1><h2>musings on tech, books and general geekery.</h2></header></section></div><section class="post" id="wrapper"><article><header><h1>Machine Learning Refresher</h1><h2 class="headline">Aug 07, 2018 4:18·
812 words
·
5 minutes read<span class="tags"></span></h2></header><section id="post-body"><p>Recently I had to brush up my machine learning knowledge. Here are my favorite resources, accompanied with some notes I took.</p>
<a id="more"></a>
<h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h2><ul>
<li><strong>Jason’s Machine Learning 101</strong><ul>
<li>Jason from Google put together <a href="https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/preview?imm_mid=0f9b7e&amp;cmp=em-data-na-na-newsltr_20171213&amp;slide=id.g168a3288f7_0_58" target="_blank" rel="noopener">a terrific slidedeck</a>, addressing all the major concepts within the AI-field. A great starter.</li>
</ul>
</li>
<li><strong>Machine Learning Crash Course</strong><ul>
<li>XYZ</li>
</ul>
</li>
</ul>
<hr>
<h2 id="My-notes"><a href="#My-notes" class="headerlink" title="My notes"></a>My notes</h2><h3 id="Jason’s-Machine-Learning-101-notes"><a href="#Jason’s-Machine-Learning-101-notes" class="headerlink" title="Jason’s Machine Learning 101 notes"></a>Jason’s Machine Learning 101 notes</h3><ul>
<li><strong>AI:</strong> “Human intelligence exhibited by machines <ul>
<li>Narrow AI: system that can just do a few defined things as well or better than humans<ul>
<li><strong>Machine learning:</strong> an approach to achieve artificial intelligence through systems that can learn from experience to find patterns in a set of data. ML is about creating algorithms that learn complex functions (or patterns) from data and make predictions - a form of narrow AI<ul>
<li><strong>Supervised learning:</strong> system is presented with labeled data</li>
<li><strong>Unsupervised learning:</strong> system must learn from unlabeled data<ul>
<li>Classification:<ul>
<li>K-nearest neightbour: clustering based on sampling the X closest observations</li>
</ul>
</li>
</ul>
</li>
<li><strong>Reinforcement learning:</strong> learning by trail-and-error through reward or punishment</li>
<li><strong>Deep learning:</strong> a technique for implementing machine learning<ul>
<li>Deep neural networks (DNNs): using a structure arranged in layers that loosely mimic the human brain, learning patterns of patterns. Uses many “hidden” layers.</li>
</ul>
</li>
<li>Splitting on too many dimensions (features) may lead to overfitting</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Regression: predicts continious values</li>
<li>Classification: predicts discrete values</li>
</ul>
<hr>
<h3 id="Machine-Learning-Crash-Course-notes"><a href="#Machine-Learning-Crash-Course-notes" class="headerlink" title="Machine Learning Crash Course notes"></a>Machine Learning Crash Course notes</h3><ul>
<li>Loss function: function aggregating individual model-vs-observation losses</li>
<li>Gradient descent:: Iterative approach to find lowest overall loss (when the model is <strong>converged</strong>)</li>
<li>Model convergence: overall loss stops changing (or at least very slow)</li>
<li>Hyperparameters: knobs to tweak machine learning algorithms<ul>
<li>Learning rate: hyperparameter that controls how much we are adjusting the weights of our model wwith respect to the loss gradient (also step size)</li>
<li>Goldilocks value: perfect value, neither too small nor too large</li>
</ul>
</li>
<li>Stochastic: having a random probability distribution or pattern </li>
<li>TensorFlow: computational framework for building machine learning models</li>
<li>steps: which is the total number of training iterations. One step calculates the loss from one batch and uses that value to modify the model’s weights once.</li>
<li>batch size: which is the number of examples (chosen at random) for a single step. For example, the batch size for SGD is 1.<ul>
<li><code>total number of trained examples = batch size * steps</code></li>
</ul>
</li>
</ul>
<p><img src="/images/2018-08-07-10-56-52.png" alt="Trial-and-error process that machine learning algorithms use to train a model"></p>
<ul>
<li>Generalization: model’s ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model</li>
<li>Ockham’s Razor principle: the less complex a model is, the more likely that a good empirical result is not just due to peculairities of our sample </li>
<li>Overfitting: low loss during training, high loss prediciting new data</li>
<li>training set: a subset to train a model.</li>
<li>validation set: evaluate results from the training set (helps preventing overfit)</li>
<li>test set: a subset to double-check the model.</li>
</ul>
<p><img src="/images/2018-08-07-11-09-45.png" alt=""></p>
<blockquote>
<p>Debugging in ML is often data debugging rather than code debugging.</p>
</blockquote>
<ul>
<li>Feature engineering: extracting features from real life data</li>
<li>Feature vector: set of floating-point values comprising the examples in your data set</li>
<li>One-hot encoding: transform categorial data to numerical dictionairy </li>
<li>Binning trick: create several boolean bins, each mapping to a new unique feature</li>
</ul>
<p><img src="/images/2018-08-07-11-57-04.png" alt=""></p>
<h4 id="Properties-of-a-good-feature"><a href="#Properties-of-a-good-feature" class="headerlink" title="Properties of a good feature"></a>Properties of a good feature</h4><ul>
<li>Avoid rarely used discrete feature values </li>
<li>Features should have a clear, obvious meaning</li>
<li>Features shouldn’t take on “magic” values </li>
<li>Definition of features should not change over time</li>
<li>Feature should not have crazy outlier data</li>
</ul>
<h4 id="Good-habits-for-handling-data"><a href="#Good-habits-for-handling-data" class="headerlink" title="Good habits for handling data"></a>Good habits for handling data</h4><ul>
<li>Visualize: plot histograms, rank most to least common</li>
<li>Debug: duplicate examples? missing values? outliers? training/validation data similar?</li>
<li><p>Monitor: feature quantiles: number of examples over time? </p>
</li>
<li><p>Feature cross: synthetic feature formed by multiplying two or more features</p>
</li>
</ul>
<h4 id="Model-Tuning-Heuristics"><a href="#Model-Tuning-Heuristics" class="headerlink" title="Model Tuning Heuristics"></a>Model Tuning Heuristics</h4><blockquote>
<ul>
<li>Training error should steadily decrease, steeply at first, and should eventually plateau as training converges.</li>
<li>If the training has not converged, try running it for longer.</li>
<li>If the training error decreases too slowly, increasing the learning rate may help it decrease faster.<ul>
<li>But sometimes the exact opposite may happen if the learning rate is too high.</li>
</ul>
</li>
<li>If the training error varies wildly, try decreasing the learning rate.<ul>
<li>Lower learning rate plus larger number of steps or larger batch size is often a good combination.</li>
</ul>
</li>
<li>Very small batch sizes can also cause instability. First try larger values like 100 or 1000, and decrease until you see degradation.</li>
</ul>
</blockquote>
<p><em><a href="https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?utm_source=mlcc&amp;utm_campaign=colab-external&amp;utm_medium=referral&amp;utm_content=firststeps-colab&amp;hl=en#scrollTo=QU5sLyYTqzqL" target="_blank" rel="noopener">colab.research.google.com</a></em></p>
<hr>
<h3 id="Kaggle-learning-machine-learning"><a href="#Kaggle-learning-machine-learning" class="headerlink" title="Kaggle: learning machine learning"></a>Kaggle: learning machine learning</h3><p>xyz</p>
<hr>
<h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h2><ul>
<li>Labels: the true thing we are predicting, Y<ul>
<li>Labeled example: {features, label} -&gt; {x, y}</li>
<li>Unlabeled example: {features, ?} -&gt; {x, ?}</li>
</ul>
</li>
<li>Features / attributes: properties of things you want to learn about<ul>
<li>x<sub>1</sub>, x<sub>2</sub>, …, x<sub>n</sub></li>
</ul>
</li>
<li>Model: maps examples to predicted labels: y’. Defines the relationship between the features and label<ul>
<li>Inference: applying the model to unlabeled examples. Using the model to make predictions: y’</li>
</ul>
</li>
<li>Example: particular instance of data <strong>x</strong> </li>
<li>Model convergence: when loss function stops changing (or really slow); in other words the most efficient model parameters are found</li>
</ul>
<p><img src="/images/2018-08-06-11-11-00.png" alt="abc"></p>
<p><img src="/images/2018-08-07-11-32-32.png" alt=""></p>
<h2 id="Resources-1"><a href="#Resources-1" class="headerlink" title="Resources"></a>Resources</h2><ul>
<li><a href="https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/preview?imm_mid=0f9b7e&amp;cmp=em-data-na-na-newsltr_20171213&amp;slide=id.g168a3288f7_0_58" target="_blank" rel="noopener">sdf</a></li>
<li><a href="https://developers.google.com/machine-learning/crash-course/ml-intro" target="_blank" rel="noopener">#</a></li>
<li><a href="https://developers.google.com/machine-learning/glossary" target="_blank" rel="noopener">ML Glossary</a></li>
<li><a href="https://www.kaggle.com/learn/machine-learning" target="_blank" rel="noopener">Kaggle: learn machine learning</a></li>
</ul>
</section><nav id="post-nav"><span class="prev"><a href="/2018/08/09/life-algorithms/"><span class="arrow">←</span>Newer Posts</a></span><span class="next"><a href="/2018/08/02/hello-world/">Older Posts<span class="arrow">→</span></a></span></nav></article></section><footer id="footer"><div id="social"><a class="symbol" href="https://github.com/fuzhouxxdong"><i class="fa fa-github"></i></a></div><p class="small">niekdewin.com 2018  </p></footer><script>hljs.initHighlightingOnLoad();</script></body></html>